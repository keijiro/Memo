# Dcam and Dcam2

Stable Diffusion ベースの映像システムを構築し Channel #23 で使用した。また、更に改良したバージョンを Channel #24 で使用した。

## 概要

[Apple が開発している Stable Diffusion の Core ML 移植版](https://github.com/apple/ml-stable-diffusion) を使用し、カメラから入力された映像をセミリアルタイムに image-to-image 変換するシステムを構築した。

この image-to-image パイプラインでは、１フレームの変換に最低でも 1.5 秒ほどかかる。この待ち時間を如何に隠すか、というのが課題となる。

今回は flip book 風のアニメーションで間を繋ぎ、この待ち時間を感じさせない工夫とした。

## 取り組んだ課題と解決法

### ハードウェアの選択

**課題** - Stable Diffusion を動作させるための PC を一人で運搬・運用するのは困難。

**解決** - Apple の Core ML Stable Diffusion を導入することで MacBook Pro １台での運用が可能になった。

### プラグインの開発

**課題** - Core ML Stable Diffusion は Swift で書かれており、ノウハウの少ない Swift でのネイティブプラグイン実装を行う必要がある。

**解決** - [簡単なプロジェクト](https://github.com/keijiro/UnitySwiftPluginTest)から始め、ノウハウを蓄積した。

### Stable Diffusion の速度

**課題** - Stable Diffusion の image-to-image 変換には最低でも 1.5 秒かかる。

**解決** - Flip book アニメーションを使ってこれを表現の中に組み込んだ。

### カメラ入力

**課題** - カメラから映像をいかに入力するか。また、カメラ操作は誰が行うか？

**解決** - NDI を使用してリモート操作を可能にした。撮影と操作を iPhone から遠隔で行うことができる。

## 改良

### sd-turbo 化

ml-stable-diffusion に sd-turbo (LCM) 対応の追加を試みている人が複数おり、これらの fork を参考にして、[sd-turbo 対応を実現できた](https://github.com/keijiro/ml-stable-diffusion)。

sd-turbo の導入により、推論の step 数を２にまで減らすことが可能になった。実際には step=2 では制御の難しい部分があり、step=4 での運用が基本になる。

Step=2 での推論は 0.8 秒程度、step=4 では 1.1 秒程度に収まる。

sd-turbo の欠点は strength の制御がほぼ効かなくなるという点。これについては、元々 strength 制御はそれほど使っていなかったと割り切ることにした。Strength=50% での運用が基本になる。Step=4 ならオプションとして Strength=75% も選択できるが、かなり大幅に絵を変えてしまうので、エフェクトとしては使いにくくなる。

### iPhone の接続

以前の運用では Wi-Fi による無線接続や USB extender を使った有線接続などを試したが、どれも一長一短があった。USB 有線接続は鉄板のように思われて、実際には初期接続に失敗する場合もあり、万全の手段ではない。

結果、有線 LAN を使った接続に落ち着いた。幸いなことに iPhone が USB-C 対応したことによって、有線 LAN 接続のハードルはかなり下がった。ケーブルの選択肢も多く、長距離の伝送にも対応できることから、これが最終的な解答になるのではないかと思われる。

## Dcam2

上記の改良を取り組みつつ、更に表現の幅を拡張したバージョンとして Dcam2 を開発した。

### 改良点

- BodyPix の並列運用
- 人体ステンシルを使用した合成エフェクトの追加
- マルチパス化による演出の余地の創出
- VFX の追加
- パラパラアニメーションの改良
- Unity 6 への移行
- UI Toolkit への移行

#### BodyPix の並列運用

Sentis を使用して BodyPix を組み込んだ。これにより人体ステンシル（マスク）とキーポイントの抽出が可能になった。これらの情報は主に VFX で利用する。

#### 人体ステンシルを使用した合成エフェクトの追加

人体ステンシルを使用して背景・前景の区別が付けられるようになった。これにより、前景の後ろにエフェクトが隠れるという表現が可能になり、演出に奥行き感が生まれた。また、背景と前景を分離して別々にポスタライズエフェクトをかけられるようにした。

#### マルチパス化による演出の余地の創出

Dcam ではワンパス（シングルカメラ）にまとめていたため、エフェクトの組み込みにかなりの制約があった。今回は正攻法的にパスを分ける構成にした（プレフィルタ、VFX、パラパラアニメーション、が完全に分離している）ため、それぞれのパスにエフェクトを組み込むことが容易になった。

#### VFX の追加

上記の要素を活かして様々な VFX を新たに組み込んだ。

#### パラパラアニメーションの改良

パラパラが徐々にスローダウンして、最終的に止め絵になる、というアニメーションを構築した。見た目での効果は分かりにくかったが、動きに抑揚（緩急）が生まれることが、最終的な見た目の面白さに僅かながらも貢献していたように思える。

また、カメラの制御についても、前作よりも動きの大きなものにした。

#### Unity 6 への移行

あまり大きな変更は無かったが、とりあえず Unity 6 にアップグレードした。

#### UI Toolkit への移行

Controller の UI を UI Toolkit に変更した。これも大きな変更では無いが、最終的な調整はしやすかった気がする。特に uxml をテキストエディタでも編集できるのが存外に嬉しかった。

### 実際の運用から得た課題

有線 LAN 接続は USB 接続よりも安定しているように思えるが、それでもなかなか初期化が通らないことがある。できれば原因を特定して改善したい。また、コントローラーアプリ側もバックグラウンドからのレジューム処理をもっと適切に実装するべきかもしれない。

Stable Diffusion パイプラインから出力される画像がおかしくなることがあった。真っ黒のブランク画像になったり、デノイズ途中の中間状態のような画像が出力されることもあった。アプリの再起動でも治らないことがあり、OS の再起動が必要とされるようだった。

今は本番直前に再起動を挟むことによって問題を回避できているように思われる。しかしかなり危険な要素であることは間違いない。できれば原因を特定し修正したいが、「再起動で治る」という点からシステム側の問題である可能性も懸念される。

GitHub Universe Recap では本番直前に Controller の接続に失敗するというトラブルが発生した。NDI のコネクションは成立していたようだが、画面が真っ黒になっていた。原因は不明。リブート等の対処でも効果は無く、Controller プロジェクトをリビルドして再インストールすることでようやく治った。

原因は何だろう？ Controller 側の画面にはカメラの映像が映っていたので、カメラパーミッションなどの問題ではないことは自明。 NDI プラグインが何らかの理由で映像の送信に失敗していたと考えるのが自然か？ LAN アダプタの問題だろうか？ リビルド＆再インストール時に USB ケーブルの抜き差しを何回か行うことになるので、そのタイミングで何かが復活した、と考えることはできるだろうか？
